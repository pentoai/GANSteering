{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.4\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this if you are in google colab\n",
    "\n",
    "# %%bash\n",
    "# git clone https://github.com/fengqingthu/CLIP_Steering.git\n",
    "# git clone https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only run this if you are in google colab\n",
    "# %%bash\n",
    "\n",
    "# pip install ninja 2>> install.log\n",
    "# git clone https://github.com/SIDN-IAP/global-model-repr.git tutorial_code 2>> install.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this if you are in google colab\n",
    "# %%bash\n",
    "# pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# pip install \\\n",
    "#   pytorch-pretrained-biggan \\\n",
    "#   ftfy \\\n",
    "#   regex \\\n",
    "#   tqdm \\\n",
    "#   git+https://github.com/openai/CLIP.git \\\n",
    "#   click \\\n",
    "#   requests \\\n",
    "#   pyspng \\\n",
    "#   ninja \\\n",
    "#   imageio-ffmpeg==0.4.3 \\\n",
    "#   scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import google.colab\n",
    "import sys, torch\n",
    "\n",
    "sys.path.append('tutorial_code')\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Change runtime type to include a GPU.\")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all dependencies\n",
    "\n",
    "Make sure to add CLIP_steering into the path so that we can use the GANAlyze tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pytorch 1.9.1+cu111 using cuda\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import clip\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.hub\n",
    "from netdissect import proggan\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.insert(0, \"./CLIP_Steering\")\n",
    "try:\n",
    "    import ganalyze_common_utils as common\n",
    "    import ganalyze_transformations as transformations\n",
    "except ImportError:\n",
    "    print(\"Could not import ganalyze_common_utils or ganalyze_transformations\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running pytorch', torch.__version__, 'using', device.type)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and test the Pro GAN model\n",
    "\n",
    "The GAN generator is just a function z->x that transforms random z to realistic images x.\n",
    "\n",
    "To generate images, all we need is a source of random z.  Let's make a micro dataset with a few random z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"http://gandissect.csail.mit.edu/models/proggan_bedroom-d8a89ff1.pth\" to /home/ubuntu/.cache/torch/hub/checkpoints/proggan_bedroom-d8a89ff1.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127f4c2aa6724f0ebfa54012e3443c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/70.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     sd \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhub\u001b[39m.\u001b[39mmodel_zoo\u001b[39m.\u001b[39mload_url(url) \u001b[39m# pytorch 1.0\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m proggan_model \u001b[39m=\u001b[39m proggan\u001b[39m.\u001b[39;49mfrom_state_dict(sd)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     19\u001b[0m proggan_model\n",
      "File \u001b[0;32m~/GANSteering/proggan/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:852\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    849\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    850\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 852\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/GANSteering/proggan/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:530\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    529\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 530\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    532\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    533\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    534\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    535\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/GANSteering/proggan/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:530\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    529\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 530\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    532\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    533\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    534\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    535\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/GANSteering/proggan/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:552\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39mif\u001b[39;00m param \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    550\u001b[0m     \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 552\u001b[0m         param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    553\u001b[0m     should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    554\u001b[0m     \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/GANSteering/proggan/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:850\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    848\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    849\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 850\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch.hub\n",
    "from netdissect import nethook, proggan\n",
    "\n",
    "n = 'proggan_bedroom-d8a89ff1.pth'\n",
    "# n = 'proggan_churchoutdoor-7e701dd5.pth'\n",
    "# n = 'proggan_conferenceroom-21e85882.pth'\n",
    "# n = 'proggan_diningroom-3aa0ab80.pth'\n",
    "# n = 'proggan_kitchen-67f1e16c.pth'\n",
    "# n = 'proggan_livingroom-5ef336dd.pth'\n",
    "# n = 'proggan_restaurant-b8578299.pth'\n",
    "\n",
    "url = 'http://gandissect.csail.mit.edu/models/' + n\n",
    "try:\n",
    "    sd = torch.hub.load_state_dict_from_url(url) # pytorch 1.1\n",
    "except:\n",
    "    sd = torch.hub.model_zoo.load_url(url) # pytorch 1.0\n",
    "proggan_model = proggan.from_state_dict(sd).to(device)\n",
    "proggan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import zdataset,renormalize\n",
    "\n",
    "SAMPLE_SIZE = 50 # Increase this for better results (but slower to run)\n",
    "zds = zdataset.z_dataset_for_model(proggan_model, size=SAMPLE_SIZE, seed=5555)\n",
    "len(zds), zds[0][0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and test CLIP model\n",
    "\n",
    "Check that the CLIP model works fine. We import CLIP by installing it through PIP. We cloned CLIP's repo to get the CLIP/CLIP.png test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model.eval()\n",
    "clip_model.to(device)\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP/CLIP.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a livingroom\", \"a bedroom\", \"a church\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.encode_image(image)\n",
    "    text_features = clip_model.encode_text(text)\n",
    "\n",
    "    logits_per_image, logits_per_text = clip_model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "latent_space_dim = zds[0][0][:,0,0].shape[0]\n",
    "context_length = clip_model.context_length\n",
    "vocab_size = clip_model.vocab_size\n",
    "\n",
    "print(\n",
    "    \"Model parameters:\",\n",
    "    f\"{np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\",\n",
    ")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Latent space dimension:\", latent_space_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "from typing import List, Optional, Tuple\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def show_images(\n",
    "        images: list[Image.Image],\n",
    "        resize: Optional[Tuple[int, int]] = None\n",
    "    ):\n",
    "    \"\"\"Show a list of images in a row.\"\"\"\n",
    "    images = [np.array(img) for img in images]\n",
    "    images = np.concatenate(images, axis=1)\n",
    "    images = Image.fromarray(images)\n",
    "\n",
    "    if resize:\n",
    "        images.thumbnail(resize)\n",
    "\n",
    "    IPython.display.display(images)\n",
    "\n",
    "\n",
    "def show_and_save_images(\n",
    "    images: list[Image.Image], batch: int, path: str, variant: str = \"original\"\n",
    "):\n",
    "    show_images(images)\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        img.save(f\"{path}/image_{batch}_{i}_{variant}.png\")\n",
    "\n",
    "\n",
    "def show_gan_results(gan_results: List[List[Tuple[Image.Image, np.ndarray]]]):\n",
    "    for batch_results in gan_results:\n",
    "        batch_size = len(batch_results[0][0])\n",
    "      \n",
    "        for i in range(batch_size):\n",
    "            steering_images = [res[0][i] for res in batch_results]\n",
    "            steering_scores = np.stack(\n",
    "                [res[1][i].detach().cpu().numpy() for res in batch_results]\n",
    "            ).tolist()\n",
    "            print(steering_scores)\n",
    "            show_images(steering_images, resize=(1024, 256))\n",
    "\n",
    "def get_clip_probs(image_inputs, text_features, model, attribute_index=0):\n",
    "    image_inputs = torch.stack([preprocess(img.resize((512, 512))) for img in image_inputs]).to(device)\n",
    "    image_features = model.encode_image(image_inputs).float()\n",
    "\n",
    "    # normalized features\n",
    "    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "    # cosine similarity as logits\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "    clip_probs = logits_per_image.softmax(dim=-1)\n",
    "\n",
    "    return clip_probs.narrow(dim=-1, start=attribute_index, length=1).squeeze(dim=-1)\n",
    "\n",
    "def show_gan_results(gan_results: list):\n",
    "    for batch_results in gan_results:\n",
    "        batch_size = len(batch_results[0][0])\n",
    "      \n",
    "        for i in range(batch_size):\n",
    "            steering_images = [res[0][i] for res in batch_results]\n",
    "            steering_scores = np.stack(\n",
    "                [res[1][i].detach().cpu().numpy() for res in batch_results]\n",
    "            ).tolist()\n",
    "            print(steering_scores)\n",
    "            show_images(steering_images, resize=(1024, 256))\n",
    "\n",
    "\n",
    "def make_images_and_probs(\n",
    "    model, zdataset, clip_model, encoded_text, attribute_index=0\n",
    "):\n",
    "    gan_images = []\n",
    "    for z in zdataset:\n",
    "      gan_output = model(z[None,...])[0]      \n",
    "      gan_images.append(renormalize.as_image(gan_output))\n",
    "      \n",
    "    clip_probs = get_clip_probs(gan_images, encoded_text, clip_model, attribute_index)\n",
    "\n",
    "    return gan_images, clip_probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CLIP to extract target text attributes\n",
    "\n",
    "Use the CLIP model to extract target text attributes for steering the output of a GAN. We use the CLIP tokenizer and encoder to extract text features and normalize them.\n",
    "\n",
    "The resulting text features are used later to steer the GAN output towards the desired attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m text_descriptions \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m attributes]\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 10\u001b[0m     text_tokens \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39;49mtokenize(text_descriptions)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     11\u001b[0m     text_features \u001b[39m=\u001b[39m clip_model\u001b[39m.\u001b[39mencode_text(text_tokens)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     12\u001b[0m     \u001b[39m# text_features = F.normalize(text_features, p=2, dim=-1)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# Extract text features for clip\n",
    "\n",
    "# Here is how we specify the desired attributes\n",
    "attributes = [\"a luxurious bedroom\", \"a bedroom\"]\n",
    "attribute_index = 0  # which attribute do we want to maximize\n",
    "text_descriptions = [f\"{label}\" for label in attributes]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_tokens = clip.tokenize(text_descriptions).to(device)\n",
    "    text_features = clip_model.encode_text(text_tokens).float()\n",
    "    # text_features = F.normalize(text_features, p=2, dim=-1)\n",
    "\n",
    "text_features.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare the GAN streering model\n",
    "\n",
    "This is the model in charge of changing the vectors `z` so that it aligns with the objective declared in `text_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = transformations.OneDirection(latent_space_dim, vocab_size)\n",
    "transformation = transformation.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = f\"checkpoints/results_maximize_classifier_probability\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_name = f\"pytorch_model_progran_steering_{attributes[attribute_index]}_final.pth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning steps\n",
    "\n",
    "Now we're ready to train the GAN steering model called `transformation`. The overall algorithm looks like this:\n",
    "\n",
    "1. Generate the noise and class vectors for a given number of training samples.\n",
    "2. For each batch, generate the GAN images and calculate the CLIP scores comparing the images features correlation with the target text features.\n",
    "3. Use the `transformation` model to adjust the original noise. Repeat step 2 for the transformed noise `z_transformed`.\n",
    "4. Compare the scoring output, and make the model optimize to minimize the difference between the target scores and the resulting one after transforming the original noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformation.parameters(), lr=0.0002\n",
    ")  # as specified in GANalyze\n",
    "losses = common.AverageMeter(name=\"Loss\")\n",
    "\n",
    "#  training settings\n",
    "optim_iter = 0\n",
    "batch_size = 64  # Do not change\n",
    "train_alpha_a = -0.5  # Lower limit for step sizes\n",
    "train_alpha_b = 0.5  # Upper limit for step sizes\n",
    "#\n",
    "# Number of samples to train for # Ganalyze uses 400,000 samples.\n",
    "# Use smaller number for testing.\n",
    "#\n",
    "num_samples = 90_000\n",
    "\n",
    "attribute_index = 0\n",
    "checkpoint_dir = f\"checkpoints/results_maximize_{attributes[attribute_index]}_probability\"\n",
    "pathlib.Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "zds = zdataset.z_dataset_for_model(proggan_model, size=num_samples, seed=5555)\n",
    "zds_dataloader = DataLoader(zds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "progress = tqdm(\"Training\", total=len(range(0, num_samples, batch_size)))\n",
    "\n",
    "# loop over data batches\n",
    "for batch_start, z_batch in enumerate(zds_dataloader):\n",
    "    z_batch = z_batch[0].squeeze().to(device)\n",
    "\n",
    "    step_sizes = (train_alpha_b - train_alpha_a) * np.random.random(\n",
    "        size=(batch_size)\n",
    "    ) + train_alpha_a  # sample step_sizes\n",
    "\n",
    "    step_sizes_broadcast = np.repeat(step_sizes, latent_space_dim).reshape(\n",
    "        [batch_size, latent_space_dim]\n",
    "    )\n",
    "    step_sizes_broadcast = (\n",
    "        torch.from_numpy(step_sizes_broadcast).type(torch.FloatTensor).to(device)\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Generate the original images and get their clip scores\n",
    "    #\n",
    "    gan_images, out_scores = make_images_and_probs(\n",
    "        model=proggan_model,\n",
    "        zdataset = z_batch,\n",
    "        clip_model=clip_model,\n",
    "        encoded_text=text_features,\n",
    "        attribute_index=attribute_index,\n",
    "    )\n",
    "\n",
    "    # TODO: ignore z vectors with less confident clip scores\n",
    "    target_scores = torch.clip(\n",
    "        out_scores + torch.from_numpy(step_sizes).to(device).float(),\n",
    "        0.0,\n",
    "        1.0\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Transform the z vector and get the clip scores for the transformed images\n",
    "    #\n",
    "    zb_transformed = transformation.transform(z_batch, None, step_sizes = step_sizes_broadcast)\n",
    "    gan_images_transformed, out_scores_transformed = make_images_and_probs(\n",
    "        model=proggan_model,\n",
    "        zdataset = zb_transformed,\n",
    "        clip_model=clip_model,\n",
    "        encoded_text=text_features,\n",
    "        attribute_index=attribute_index,\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Compute loss and backpropagate\n",
    "    #\n",
    "    loss = transformation.criterion(out_scores_transformed, target_scores)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #\n",
    "    # Print and save intermediate results\n",
    "    #\n",
    "    losses.update(loss.item(), batch_size)\n",
    "    if optim_iter % 50 == 0:\n",
    "        print(\n",
    "            f\"[Maximizing score for {attributes[attribute_index]}] \"\n",
    "            f\"Progress: [{batch_start}/{num_samples}] {losses}\"\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"[Scores] \"\n",
    "            f\"Target: {target_scores} Out: {out_scores_transformed}\"\n",
    "        )\n",
    "\n",
    "    if optim_iter % 200 == 0:\n",
    "        batch_checkpoint_name = f\"pytorch_model_progran_{batch_start}.pth\"\n",
    "        torch.save(\n",
    "            transformation.state_dict(),\n",
    "            os.path.join(checkpoint_dir, batch_checkpoint_name)\n",
    "        )\n",
    "\n",
    "        # plot and save sample images\n",
    "        # show_and_save_images(gan_images, batch_start, checkpoint_dir)\n",
    "        # show_and_save_images(\n",
    "        #     gan_images_transformed, batch_start, checkpoint_dir, \"transformed\"\n",
    "        # )\n",
    "\n",
    "    optim_iter = optim_iter + 1\n",
    "    progress.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "torch.save(\n",
    "    transformation.state_dict(),\n",
    "    checkpoint_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this if you are in google colab\n",
    "# !rm -rf Weights\n",
    "# !mkdir -p Weights\n",
    "\n",
    "# # mount files from google drive\n",
    "# # and follow the steps here\n",
    "# # from google.colab import drive\n",
    "# # drive.mount('/content/gdrive')\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# shutil.copy(checkpoint_path, f\"/content/gdrive/MyDrive/Colab Notebooks/Sabrina <> Leo/Weights/{checkpoint_name}\")\n",
    "# shutil.copy(checkpoint_path, f\"Weights/{checkpoint_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing steps\n",
    "\n",
    "Now that the model is trained, we can test it and see how the output changes when incrementing and decrementing the step_sizes on our `z` vectors.\n",
    "\n",
    "We take the latest saved checkpoint located at `{checkpoint_dir}/pytorch_model_final.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = transformations.OneDirection(latent_space_dim)\n",
    "transformation.load_state_dict(\n",
    "    torch.load(\n",
    "        os.path.join(checkpoint_dir, checkpoint_name),\n",
    "    ),\n",
    "    strict=True,\n",
    ")\n",
    "transformation.to(device)\n",
    "transformation.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now that the model is trained, we can test it.\n",
    "#\n",
    "# Testing the model involves using the transformation model to transform a z vector and then\n",
    "# using the GAN model to generate an image from the transformed z vector.\n",
    "# We will change the step size and see how the image changes.\n",
    "#\n",
    "batch_size = 6  # Do not change\n",
    "alpha = 0.2\n",
    "num_samples = 6\n",
    "\n",
    "iters = 10\n",
    "\n",
    "transformation = transformations.OneDirection(latent_space_dim)\n",
    "transformation.load_state_dict(\n",
    "    torch.load(\n",
    "        os.path.join(\"Weights\", checkpoint_name),\n",
    "    ),\n",
    "    strict=True,\n",
    ")\n",
    "transformation.to(device)\n",
    "transformation.eval()\n",
    "\n",
    "gan_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    zds = zdataset.z_dataset_for_model(proggan_model, size=num_samples, seed=5555)\n",
    "    zds_dataloader = DataLoader(zds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    progress = tqdm(\"Training\", total=len(range(0, num_samples, batch_size)))\n",
    "    \n",
    "    # loop over data batches\n",
    "    for batch_start, z_batch in enumerate(zds_dataloader):\n",
    "        #\n",
    "        # Setup the batch z and y vectors. Also sample step sizes.\n",
    "        #\n",
    "        z_batch = z_batch[0].squeeze().to(device)\n",
    "        \n",
    "        step_sizes = (\n",
    "            (torch.ones((batch_size, latent_space_dim)) * alpha).float().to(device)\n",
    "        )\n",
    "\n",
    "        gan_images, out_scores = make_images_and_probs(\n",
    "            model=proggan_model,\n",
    "            zdataset = z_batch,\n",
    "            clip_model=clip_model,\n",
    "            encoded_text=text_features,\n",
    "            attribute_index=attribute_index,\n",
    "        )\n",
    "\n",
    "        batch_results = [(gan_images, out_scores)]\n",
    "\n",
    "        # Generate images by transforming the z vector in the negative direction\n",
    "        z_negative = z_batch.clone()\n",
    "\n",
    "        for iter in range(iters):\n",
    "            z_negative = transformation.transform(z_negative, None, -step_sizes)\n",
    "            batch_results.insert(\n",
    "                0,\n",
    "                make_images_and_probs(\n",
    "                    model=proggan_model,\n",
    "                    zdataset = z_negative,\n",
    "                    clip_model=clip_model,\n",
    "                    encoded_text=text_features,\n",
    "                    attribute_index=attribute_index,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Generate images by transforming the z vector in the positive direction\n",
    "        z_positive = z_batch.clone()\n",
    "\n",
    "        for iter in range(iters):\n",
    "            z_positive = transformation.transform(z_positive, None, step_sizes)\n",
    "            batch_results.append(\n",
    "                make_images_and_probs(\n",
    "                    model=proggan_model,\n",
    "                    zdataset = z_positive,\n",
    "                    clip_model=clip_model,\n",
    "                    encoded_text=text_features,\n",
    "                    attribute_index=attribute_index,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        gan_results.append(batch_results)\n",
    "\n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gan_results(gan_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooking a model with InstrumentedModel\n",
    "\n",
    "To analyze what a model is doing inside, we can wrap it with an InstrumentedModel, which makes it easy to hook or modify a particular layer.\n",
    "\n",
    "InstrumentedModel adds a few useful functions for inspecting a model, including:\n",
    "   * `model.retain_layer('layername')` - hooks a layer to hold on to its output after computation\n",
    "   * `model.retained_layer('layername')` - returns the retained data from the last computation\n",
    "   * `model.edit_layer('layername', rule=...)` - runs the `rule` function after the given layer\n",
    "   * `model.remove_edits()` - removes editing rules\n",
    "\n",
    "Let's setup `retain_layer` now.  We'll pick a layer sort of in the early-middle of the generator.  You can pick whatever you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import nethook\n",
    "\n",
    "# Don't re-wrap it, if it's already wrapped (e.g., if you press enter twice)\n",
    "if not isinstance(proggan_model, nethook.InstrumentedModel):\n",
    "    proggan_model = nethook.InstrumentedModel(proggan_model)\n",
    "proggan_model.retain_layer('layer4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "img = proggan_model(zds[0][0][None,...].to(device))\n",
    "\n",
    "# As a side-effect, the proggan_model has retained the output of layer4.\n",
    "acts = proggan_model.retained_layer('layer4')\n",
    "\n",
    "# We can look at it.  How much data is it?\n",
    "acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just look at the 0th convolutional channel.\n",
    "print(acts[0,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing activation data\n",
    "\n",
    "It can be informative to visualize activation data instead of just looking at the numbers.\n",
    "\n",
    "Net dissection comes with an ImageVisualizer object for visualizing grid data as an image in a few different ways.  Here is a heatmap of the array above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import imgviz\n",
    "iv = imgviz.ImageVisualizer(100)\n",
    "iv.heatmap(acts[0,1], mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import show\n",
    "\n",
    "show(\n",
    "    [['unit %d' % u,\n",
    "      [iv.image(img[0])],\n",
    "      [iv.masked_image(img[0], acts, (0,u))],\n",
    "      [iv.heatmap(acts, (0,u), mode='nearest')],\n",
    "     ] for u in range(1, 6)]  \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting quantile statistics for every unit\n",
    "\n",
    "We want to know per-channel minimum or maximum values, means, medians, quantiles, etc.\n",
    "\n",
    "We want to treat each pixel as its own sample for all the channels.  For example, here are the activations for one image as an 8x8 tensor over with 512 channels.  We can disregard the geometry and just look at it as a 64x512 sample matrix, that is 64 samples of 512-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acts.shape)\n",
    "print(acts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1]).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net dissection has a tally package that tracks quantiles over large samples.\n",
    "\n",
    "To use it, just define a function that returns sample matrices like the 64x512 above, and then it will call your function on every batch and tally up the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import tally\n",
    "\n",
    "# To collect stats, define a function that returns 2d [samples, units]\n",
    "def compute_samples(zbatch):\n",
    "    _ = proggan_model(zbatch.to(device))          # run the proggan_model\n",
    "    acts = proggan_model.retained_layer('layer4') # get the activations, and flatten\n",
    "    return acts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
    "\n",
    "# Then tally_quantile will run your function over the whole dataset to collect quantile stats\n",
    "rq = tally.tally_quantile(compute_samples, zds)\n",
    "\n",
    "# Print out the median value for the first 20 channels\n",
    "rq.quantiles(0.5)[:20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring quantiles\n",
    "\n",
    "The rq object tracks a sketch of all the quantiles of the sampled data.  For example, what is the mean, median, and percentile value for each unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells me now, for example, what the means are for channel,\n",
    "# rq.mean()\n",
    "# what median is,\n",
    "# rq.quantiles(0.5)\n",
    "# Or what the 99th percentile quantile is.\n",
    "# rq.quantiles(0.99)\n",
    "\n",
    "(rq.quantiles(0.8) > 0).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantiles can be plugged directly into the ImageVisualizer to put heatmaps on an informative per-unit scale.  When you do this:\n",
    "\n",
    "   * Heatmaps are shown on a scale from black to white from 1% lowest to the 99% highest value.\n",
    "   * Masked image lassos are shown at a 95% percentile level (by default, can be changed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv = imgviz.ImageVisualizer(100, quantiles=rq)\n",
    "show([\n",
    "    [  # for every unit, make a block containing\n",
    "       'unit %d' % u,         # the unit number\n",
    "       [iv.image(img[0])],    # the unmodified image\n",
    "       [iv.masked_image(img[0], acts, (0,u))], # the masked image\n",
    "       [iv.heatmap(acts, (0,u), mode='nearest')], # the heatmap\n",
    "    ]\n",
    "    for u in range(1, 6)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_max(zbatch):\n",
    "    image_batch = proggan_model(zbatch.to(device))\n",
    "    return proggan_model.retained_layer('layer4').max(3)[0].max(2)[0]\n",
    "\n",
    "topk = tally.tally_topk(compute_image_max, zds)\n",
    "topk.result()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each unit, this function prints out unit masks from the top-activating images\n",
    "def unit_viz_row(unitnum, percent_level=0.95):\n",
    "    out = []\n",
    "    for imgnum in topk.result()[1][unitnum][:8]:\n",
    "        img = proggan_model(zds[imgnum][0][None,...].to(device))\n",
    "        acts = proggan_model.retained_layer('layer4')\n",
    "        out.append([imgnum.item(),\n",
    "                    [iv.masked_image(img[0], acts, (0, unitnum), percent_level=percent_level)],\n",
    "                   ])\n",
    "    return out\n",
    "\n",
    "show(unit_viz_row(30))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate matches with semantic concepts\n",
    "\n",
    "Do the filters match any semantic concepts?  To systematically examine this question,\n",
    "we have pretrained (using lots of labeled data) a semantic segmentation network to recognize\n",
    "a few hundred classes of objects, parts, and textures.\n",
    "\n",
    "Run the code in this section to look for matches between filters in our GAN and semantic\n",
    "segmentation clases.\n",
    "\n",
    "## Labeling semantics within the generated images\n",
    "\n",
    "Let's quantify what's inside these images by segmenting them.\n",
    "\n",
    "First, we create a segmenter network.  (We use the Unified Perceptual Parsing segmenter by Xiao, et al. (https://arxiv.org/abs/1807.10221).\n",
    "\n",
    "Note that the segmenter we use here requires a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import segmenter, setting\n",
    "\n",
    "# segmodel = segmenter.UnifiedParsingSegmenter(segsizes=[256])\n",
    "segmodel, seglabels, _ = setting.load_segmenter('netpq')\n",
    "# seglabels = [l for l, c in segmodel.get_label_and_category_names()[0]]\n",
    "print('segmenter has', len(seglabels), 'labels')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create segmentation images for the dataset.  Here tally_cat just concatenates batches of image (or segmentation) data.\n",
    "\n",
    "  * `segmodel.segment_batch` segments an image\n",
    "  * `iv.segmentation(seg)` creates a solid-color visualization of a segmentation\n",
    "  * `iv.segment_key(seg, segmodel)` makes a small legend for the segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import upsample\n",
    "from netdissect import segviz\n",
    "\n",
    "imgs = tally.tally_cat(run_model_batch, noise_vector)\n",
    "seg = tally.tally_cat(lambda img: segmodel.segment_batch(img.cuda(), downsample=1), imgs)\n",
    "\n",
    "from netdissect.segviz import seg_as_image, segment_key\n",
    "show([\n",
    "    (iv.image(imgs[i]),\n",
    "     iv.segmentation(seg[i,0]),\n",
    "     iv.segment_key(seg[i,0], segmodel)\n",
    "    )\n",
    "    for i in range(min(len(seg), 5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We upsample activations to measure them at each segmentation location.\n",
    "upfn8 = upsample.upsampler((64, 64), (8, 8)) # layer4 is resolution 8x8\n",
    "\n",
    "\n",
    "def compute_conditional_samples(zbatch):\n",
    "    zclass = one_hot_from_int(CLIP_CLASS_ID, batch_size=len(zbatch))\n",
    "    zclass = torch.from_numpy(zclass)\n",
    "    zclass = zclass.to(zbatch.device)\n",
    "\n",
    "    image_batch = proggan_model(zbatch, zclass, truncation)\n",
    "    zclass.detach().cpu()\n",
    "\n",
    "    seg = segmodel.segment_batch(image_batch, downsample=4)\n",
    "    upsampled_acts = upfn8(proggan_model.retained_layer('layer4'))\n",
    "\n",
    "    samples = tally.conditional_samples(upsampled_acts, seg)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return samples\n",
    "\n",
    "# Run this function once to sample one image\n",
    "sample = compute_conditional_samples(noise_vector[:1])\n",
    "\n",
    "# The result is a list of all the conditional subsamples\n",
    "[(seglabels[c], d.shape) for c, d in sample]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cq = tally.tally_conditional_quantile(compute_conditional_samples, noise_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional quantile statistics let us compute lots of relationships between units and visual concepts.\n",
    "\n",
    "For example, IoU is the \"intersection over union\" ratio, measuring how much overlap there is between the top few percent activations of a unit and the presence of a visual concept.  We can estimate the IoU ratio for all pairs between units and concepts with these stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_table = tally.iou_from_conditional_quantile(cq, cutoff=0.99)\n",
    "iou_table.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's view a few of the units, labeled with an associated concept, sorted from highest to lowest IoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_list = sorted(enumerate(zip(*iou_table.max(1))), key=lambda k: -k[1][0])\n",
    "\n",
    "for unit, (iou, segc) in unit_list[:5]:\n",
    "    print('unit %d: %s (iou %.2f)' % (unit, seglabels[segc], iou))\n",
    "    show(unit_viz_row(unit))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quantify the overall match between units and segmentation concepts by counting the number of units that match a segmentation concept (omitting low-scoring matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of units total:', len(unit_list))\n",
    "print('Number of units that match a segmentation concept with IoU > 0.04:',\n",
    "   len([i for i in range(len(unit_list)) if unit_list[i][1][0] > 0.04]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining units that select for nose\n",
    "\n",
    "Now let's filter just units that were labeled as 'nose' units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
